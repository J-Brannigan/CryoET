{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:32.050825Z",
     "start_time": "2024-12-03T14:11:25.985325Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "import copick\n",
    "import zarr\n",
    "\n",
    "def load_picks(picks_folder, voxel_spacing):\n",
    "    picks = {}\n",
    "    for json_file in os.listdir(picks_folder):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(picks_folder, json_file)\n",
    "            with open(json_path, 'r') as file:\n",
    "                pick_data = json.load(file)\n",
    "            picks[json_file[:-5]] = np.array([\n",
    "                [\n",
    "                    point['location']['x'] / voxel_spacing.voxel_size,\n",
    "                    point['location']['y'] / voxel_spacing.voxel_size,\n",
    "                    point['location']['z'] / voxel_spacing.voxel_size\n",
    "                ]\n",
    "                for point in pick_data['points']\n",
    "            ])\n",
    "    return picks\n",
    "\n",
    "def extract_patches(data, picks, patch_size=16):\n",
    "    patches = []\n",
    "    labels = []\n",
    "    half_size = patch_size // 2\n",
    "\n",
    "    for particle, locations in picks.items():\n",
    "        for loc in locations:\n",
    "            x, y, z = map(int, loc)\n",
    "            # Ensure patch is within bounds\n",
    "            if (x - half_size >= 0 and x + half_size < data.shape[2] and\n",
    "                y - half_size >= 0 and y + half_size < data.shape[1] and\n",
    "                z - half_size >= 0 and z + half_size < data.shape[0]):\n",
    "                patch = data[z-half_size:z+half_size,\n",
    "                             y-half_size:y+half_size,\n",
    "                             x-half_size:x+half_size]\n",
    "                patches.append(patch)\n",
    "                labels.append(particle)\n",
    "    return np.array(patches), np.array(labels)\n",
    "\n",
    "def process_dataset(config_path, dataset_type='train'):\n",
    "    copick_root = copick.from_file(config_path)\n",
    "    runs = copick_root.runs\n",
    "    run = copick_root.get_run(runs[0].name)\n",
    "    \n",
    "    voxel_spacing = run.get_voxel_spacing(10.000)\n",
    "    \n",
    "    # Access the specific tomogram\n",
    "    tomogram = voxel_spacing.get_tomogram(\"denoised\")\n",
    "    \n",
    "    # Access the Zarr data\n",
    "    zarr_store = tomogram.zarr()\n",
    "    zarr_group = zarr.open(zarr_store)\n",
    "    \n",
    "    # Load the tomogram data\n",
    "    tomogram_vals = zarr_group['0']  # Adjust the key if needed\n",
    "    \n",
    "    # Path to the Picks folder\n",
    "    picks_folder = os.path.join(\n",
    "        '/Users/jake.brannigan/Documents/Kaggle/CryoET/Data/czii-cryo-et-object-identification',\n",
    "        dataset_type,\n",
    "        'overlay',\n",
    "        'ExperimentRuns',\n",
    "        'TS_5_4',\n",
    "        'Picks'\n",
    "    )\n",
    "    \n",
    "    picks = load_picks(picks_folder, voxel_spacing)\n",
    "    patches, labels = extract_patches(tomogram_vals, picks, patch_size=16)\n",
    "    \n",
    "    return patches, labels\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:43.798556Z",
     "start_time": "2024-12-03T14:11:40.627029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to your copick configuration\n",
    "config_path = '../../copick_config.json'\n",
    "\n",
    "# Process training data\n",
    "train_patches, train_labels = process_dataset(config_path, dataset_type='train')\n",
    "\n",
    "# Normalize patches\n",
    "train_patches = train_patches / np.max(train_patches)\n",
    "\n",
    "# Encode labels\n",
    "label_map = {name: idx for idx, name in enumerate(set(train_labels))}\n",
    "encoded_train_labels = np.array([label_map[label] for label in train_labels])\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_patches, encoded_train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Add a channel dimension for CNN\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]"
   ],
   "id": "83c0a182a620fdce",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:37.894348Z",
     "start_time": "2024-12-03T14:12:30.562355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the 3D CNN model\n",
    "model = Sequential([\n",
    "    Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(16, 16, 16, 1)),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_map), activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_val, y_val), \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "# model.save('particle_detection_model.h5')"
   ],
   "id": "1f8122bdc5c147e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 100ms/step - accuracy: 0.2393 - loss: 1.7408 - val_accuracy: 0.6071 - val_loss: 1.4230\n",
      "Epoch 2/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step - accuracy: 0.3621 - loss: 1.5309 - val_accuracy: 0.6429 - val_loss: 1.2745\n",
      "Epoch 3/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 78ms/step - accuracy: 0.4526 - loss: 1.4058 - val_accuracy: 0.6071 - val_loss: 1.1371\n",
      "Epoch 4/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 78ms/step - accuracy: 0.6124 - loss: 1.1601 - val_accuracy: 0.7500 - val_loss: 0.9877\n",
      "Epoch 5/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - accuracy: 0.7186 - loss: 1.0318 - val_accuracy: 0.6786 - val_loss: 0.8279\n",
      "Epoch 6/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step - accuracy: 0.7560 - loss: 0.8568 - val_accuracy: 0.7857 - val_loss: 0.6869\n",
      "Epoch 7/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 76ms/step - accuracy: 0.7284 - loss: 0.7470 - val_accuracy: 0.7857 - val_loss: 0.6051\n",
      "Epoch 8/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 78ms/step - accuracy: 0.7102 - loss: 0.6934 - val_accuracy: 0.7500 - val_loss: 0.5982\n",
      "Epoch 9/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step - accuracy: 0.7981 - loss: 0.5826 - val_accuracy: 0.7500 - val_loss: 0.6051\n",
      "Epoch 10/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step - accuracy: 0.8210 - loss: 0.5017 - val_accuracy: 0.7500 - val_loss: 0.6098\n",
      "Epoch 11/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step - accuracy: 0.7841 - loss: 0.6044 - val_accuracy: 0.7143 - val_loss: 0.5873\n",
      "Epoch 12/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 101ms/step - accuracy: 0.8148 - loss: 0.4885 - val_accuracy: 0.7857 - val_loss: 0.5248\n",
      "Epoch 13/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step - accuracy: 0.9215 - loss: 0.3873 - val_accuracy: 0.7500 - val_loss: 0.5107\n",
      "Epoch 14/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step - accuracy: 0.8392 - loss: 0.4809 - val_accuracy: 0.7143 - val_loss: 0.5317\n",
      "Epoch 15/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step - accuracy: 0.8798 - loss: 0.3617 - val_accuracy: 0.7857 - val_loss: 0.5526\n",
      "Epoch 16/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step - accuracy: 0.8897 - loss: 0.3486 - val_accuracy: 0.7857 - val_loss: 0.5287\n",
      "Epoch 17/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step - accuracy: 0.9178 - loss: 0.2427 - val_accuracy: 0.7500 - val_loss: 0.6298\n",
      "Epoch 18/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 78ms/step - accuracy: 0.9194 - loss: 0.2944 - val_accuracy: 0.7857 - val_loss: 0.5885\n",
      "Epoch 19/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step - accuracy: 0.9121 - loss: 0.2664 - val_accuracy: 0.7857 - val_loss: 0.4918\n",
      "Epoch 20/20\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step - accuracy: 0.9292 - loss: 0.2368 - val_accuracy: 0.7500 - val_loss: 0.5866\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:22:28.565246Z",
     "start_time": "2024-12-03T14:22:28.553843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sliding_window_predict(model, tomogram, patch_size=16, step=8, threshold=0.5, label_map=None):\n",
    "    \"\"\"\n",
    "    Performs sliding window predictions on the test tomogram.\n",
    "\n",
    "    Args:\n",
    "        model (Keras model): Trained CNN model.\n",
    "        tomogram (numpy array): 3D tomogram volume.\n",
    "        patch_size (int): Size of cubic patches.\n",
    "        step (int): Step size for sliding window.\n",
    "        threshold (float): Confidence threshold for predictions.\n",
    "        label_map (dict): Map of label indices to particle names.\n",
    "\n",
    "    Returns:\n",
    "        list: Predicted particles with their positions and labels.\n",
    "    \"\"\"\n",
    "    half_size = patch_size // 2\n",
    "    z_max, y_max, x_max = tomogram.shape\n",
    "\n",
    "    predictions = []\n",
    "    for z in range(half_size, z_max - half_size, step):\n",
    "        print(f'{z}/{z_max}')\n",
    "        for y in range(half_size, y_max - half_size, step):\n",
    "            for x in range(half_size, x_max - half_size, step):\n",
    "                patch = tomogram[z - half_size:z + half_size,\n",
    "                                 y - half_size:y + half_size,\n",
    "                                 x - half_size:x + half_size]\n",
    "                patch = patch[np.newaxis, ..., np.newaxis]  # Add batch and channel dimensions\n",
    "                patch = patch / np.max(patch)  # Normalize\n",
    "                \n",
    "                pred = model.predict(patch, verbose=0)\n",
    "                max_prob = np.max(pred)\n",
    "                label_idx = np.argmax(pred)\n",
    "                # print([key for key,value in label_map if value == label_idx][0])\n",
    "\n",
    "                if max_prob > threshold:\n",
    "                    predictions.append({\n",
    "                        \"position\": (x, y, z),\n",
    "                        \"label\": [key for key,value in label_map.items() if value == label_idx][0],\n",
    "                        \"confidence\": max_prob\n",
    "                    })\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def prepare_test_data(config_path, dataset_type='test'):\n",
    "    \"\"\"\n",
    "    Prepares test data for predictions.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the copick configuration file.\n",
    "        dataset_type (str): Dataset type ('test').\n",
    "\n",
    "    Returns:\n",
    "        tomogram_vals (numpy array): The test tomogram data.\n",
    "    \"\"\"\n",
    "    copick_root = copick.from_file(config_path)\n",
    "    runs = copick_root.runs\n",
    "    run = copick_root.get_run(runs[0].name)\n",
    "    \n",
    "    voxel_spacing = run.get_voxel_spacing(10.000)\n",
    "    \n",
    "    # Access the specific tomogram\n",
    "    tomogram = voxel_spacing.get_tomogram(\"denoised\")\n",
    "    \n",
    "    # Access the Zarr data\n",
    "    zarr_store = tomogram.zarr()\n",
    "    zarr_group = zarr.open(zarr_store)\n",
    "    \n",
    "    # Load the tomogram data\n",
    "    tomogram_vals = zarr_group['0']  # Adjust the key if needed\n",
    "    \n",
    "    return tomogram_vals"
   ],
   "id": "93238badafc3e9ba",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:14:16.735329Z",
     "start_time": "2024-12-03T14:14:16.729940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_predictions(predictions, output_path):\n",
    "    \"\"\"\n",
    "    Saves predictions in overlay format.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): List of predicted particles with positions and labels.\n",
    "        output_path (str): Path to save the JSON file.\n",
    "    \"\"\"\n",
    "    overlay_data = {\n",
    "        \"points\": [\n",
    "            {\n",
    "                \"location\": {\"x\": pos[0], \"y\": pos[1], \"z\": pos[2]},\n",
    "                \"label\": label,\n",
    "                \"confidence\": confidence\n",
    "            }\n",
    "            for pred in predictions\n",
    "            for pos, label, confidence in [(pred[\"position\"], pred[\"label\"], pred[\"confidence\"])]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(overlay_data, json_file, indent=4)"
   ],
   "id": "9238bacc10e2c76b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:26:19.589774Z",
     "start_time": "2024-12-03T14:22:31.568741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the trained model (if not already in memory)\n",
    "# model = load_model('particle_detection_model.h5')\n",
    "\n",
    "# Process test data\n",
    "tomogram_vals =  prepare_test_data(config_path)\n",
    "\n",
    "# Example usage\n",
    "predictions = sliding_window_predict(model, tomogram_vals, patch_size=16, step=8, threshold=0.5, label_map=label_map)\n",
    "save_predictions(predictions, 'output_predictions.json')"
   ],
   "id": "1d5631dc58e3dffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x104eaa660>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jake.brannigan/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m tomogram_vals \u001B[38;5;241m=\u001B[39m  prepare_test_data(config_path)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43msliding_window_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtomogram_vals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabel_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m save_predictions(predictions, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput_predictions.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[27], line 30\u001B[0m, in \u001B[0;36msliding_window_predict\u001B[0;34m(model, tomogram, patch_size, step, threshold, label_map)\u001B[0m\n\u001B[1;32m     27\u001B[0m patch \u001B[38;5;241m=\u001B[39m patch[np\u001B[38;5;241m.\u001B[39mnewaxis, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, np\u001B[38;5;241m.\u001B[39mnewaxis]  \u001B[38;5;66;03m# Add batch and channel dimensions\u001B[39;00m\n\u001B[1;32m     28\u001B[0m patch \u001B[38;5;241m=\u001B[39m patch \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(patch)  \u001B[38;5;66;03m# Normalize\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m max_prob \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(pred)\n\u001B[1;32m     32\u001B[0m label_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(pred)\n",
      "File \u001B[0;32m~/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:556\u001B[0m, in \u001B[0;36mTensorFlowTrainer.predict\u001B[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001B[0m\n\u001B[1;32m    554\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m epoch_iterator\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[0;32m--> 556\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mepoch_iterator\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_predict_batch_begin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    558\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mget_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:729\u001B[0m, in \u001B[0;36mTFEpochIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 729\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_epoch_iterator\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:112\u001B[0m, in \u001B[0;36mEpochIterator._enumerate_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m step, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_iterator\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_seen \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches:\n\u001B[0;32m--> 112\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_seen \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:501\u001B[0m, in \u001B[0;36mDatasetV2.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly() \u001B[38;5;129;01mor\u001B[39;00m ops\u001B[38;5;241m.\u001B[39minside_function():\n\u001B[1;32m    500\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mcolocate_with(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variant_tensor):\n\u001B[0;32m--> 501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43miterator_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOwnedIterator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    503\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.Dataset` only supports Python-style \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    504\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration in eager mode or within tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:709\u001B[0m, in \u001B[0;36mOwnedIterator.__init__\u001B[0;34m(self, dataset, components, element_spec)\u001B[0m\n\u001B[1;32m    705\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m (components \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m element_spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 709\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_next_call_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:748\u001B[0m, in \u001B[0;36mOwnedIterator._create_iterator\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    745\u001B[0m   \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fulltype\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(\n\u001B[1;32m    746\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_output_types)\n\u001B[1;32m    747\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator_resource\u001B[38;5;241m.\u001B[39mop\u001B[38;5;241m.\u001B[39mexperimental_set_type(fulltype)\n\u001B[0;32m--> 748\u001B[0m \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds_variant\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator_resource\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Kaggle/CryoET/venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3478\u001B[0m, in \u001B[0;36mmake_iterator\u001B[0;34m(dataset, iterator, name)\u001B[0m\n\u001B[1;32m   3476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[1;32m   3477\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3478\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3479\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMakeIterator\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3480\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m   3481\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
