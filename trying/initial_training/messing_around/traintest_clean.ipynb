{
 "cells": [
  {
   "cell_type": "code",
   "id": "81e972826cf44d86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:28:19.059957Z",
     "start_time": "2024-12-03T14:28:19.053071Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "import copick\n",
    "import zarr\n",
    "from tqdm import tqdm  # For progress bars"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:28:19.080308Z",
     "start_time": "2024-12-03T14:28:19.066723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_picks(picks_folder, voxel_spacing):\n",
    "    \"\"\"\n",
    "    Loads particle picks from JSON files.\n",
    "\n",
    "    Args:\n",
    "        picks_folder (str): Path to the Picks folder.\n",
    "        voxel_spacing: Voxel spacing object from copick.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with particle names as keys and numpy arrays of locations as values.\n",
    "    \"\"\"\n",
    "    picks = {}\n",
    "    for json_file in os.listdir(picks_folder):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(picks_folder, json_file)\n",
    "            with open(json_path, 'r') as file:\n",
    "                pick_data = json.load(file)\n",
    "            picks[json_file[:-5]] = np.array([\n",
    "                [\n",
    "                    point['location']['x'] / voxel_spacing.voxel_size,\n",
    "                    point['location']['y'] / voxel_spacing.voxel_size,\n",
    "                    point['location']['z'] / voxel_spacing.voxel_size\n",
    "                ]\n",
    "                for point in pick_data['points']\n",
    "            ])\n",
    "    return picks\n",
    "\n",
    "\n",
    "def extract_patches(data, picks, patch_size=16):\n",
    "    \"\"\"\n",
    "    Extracts cubic patches around particle locations.\n",
    "\n",
    "    Args:\n",
    "        data (numpy array): 3D tomogram volume.\n",
    "        picks (dict): Dictionary with particle types as keys and 3D coordinates as values.\n",
    "        patch_size (int): Size of the cubic patch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (patches, labels)\n",
    "            patches (numpy array): Extracted patches.\n",
    "            labels (list): Corresponding labels for the patches.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    labels = []\n",
    "    half_size = patch_size // 2\n",
    "\n",
    "    for particle, locations in picks.items():\n",
    "        for loc in locations:\n",
    "            x, y, z = loc.astype(int)\n",
    "            # Ensure patch is within bounds\n",
    "            if (x - half_size >= 0 and x + half_size < data.shape[2] and\n",
    "                    y - half_size >= 0 and y + half_size < data.shape[1] and\n",
    "                    z - half_size >= 0 and z + half_size < data.shape[0]):\n",
    "                patch = data[z - half_size:z + half_size,\n",
    "                        y - half_size:y + half_size,\n",
    "                        x - half_size:x + half_size]\n",
    "                patches.append(patch)\n",
    "                labels.append(particle)\n",
    "    return np.array(patches), np.array(labels)\n",
    "\n",
    "\n",
    "def process_dataset(config_path, dataset_type='train', tomogram_keys=None):\n",
    "    \"\"\"\n",
    "    Processes the dataset to extract patches and labels from specified tomograms.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the copick configuration file.\n",
    "        dataset_type (str): Dataset type ('train' or 'test').\n",
    "        tomogram_keys (list or None): List of tomogram keys to process. If None, process all.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (patches, labels)\n",
    "    \"\"\"\n",
    "    copick_root = copick.from_file(config_path)\n",
    "    runs = copick_root.runs\n",
    "    run = copick_root.get_run(runs[0].name)\n",
    "\n",
    "    voxel_spacing = run.get_voxel_spacing(10.000)\n",
    "\n",
    "    # Access the specific tomogram(s)\n",
    "    tomogram = voxel_spacing.get_tomogram(\"denoised\")\n",
    "\n",
    "    # Access the Zarr data\n",
    "    zarr_store = tomogram.zarr()\n",
    "    zarr_group = zarr.open(zarr_store)\n",
    "\n",
    "    patches = []\n",
    "    labels = []\n",
    "\n",
    "    # Path to the Picks folder\n",
    "    picks_folder_base = os.path.join(\n",
    "        '/Users/jake.brannigan/Documents/Kaggle/CryoET/Data/czii-cryo-et-object-identification',\n",
    "        dataset_type,\n",
    "        'overlay',\n",
    "        'ExperimentRuns',\n",
    "        'TS_5_4',\n",
    "        'Picks'\n",
    "    )\n",
    "    \n",
    "    picks = load_picks(picks_folder_base, voxel_spacing)\n",
    "    \n",
    "    # Determine tomogram keys\n",
    "    if tomogram_keys is None:\n",
    "        tomogram_keys = list(picks.keys())\n",
    "\n",
    "    for tomogram_key in tomogram_keys:\n",
    "        # Load the tomogram data\n",
    "        tomogram_vals = zarr_group[tomogram_key]\n",
    "\n",
    "        # Define picks folder for this tomogram (assuming separate Picks per tomogram)\n",
    "        picks_folder = os.path.join(picks_folder_base, tomogram_key)\n",
    "        if not os.path.exists(picks_folder):\n",
    "            print(f\"Picks folder {picks_folder} does not exist. Skipping tomogram {tomogram_key}.\")\n",
    "            continue\n",
    "\n",
    "        picks = load_picks(picks_folder, voxel_spacing)\n",
    "        tomogram_patches, tomogram_labels = extract_patches(tomogram_vals, picks, patch_size=16)\n",
    "\n",
    "        patches.append(tomogram_patches)\n",
    "        labels.append(tomogram_labels)\n",
    "\n",
    "    if patches:\n",
    "        patches = np.concatenate(patches, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "    else:\n",
    "        patches = np.array([])\n",
    "        labels = np.array([])\n",
    "\n",
    "    return patches, labels\n",
    "\n",
    "\n",
    "def prepare_label_maps(labels):\n",
    "    \"\"\"\n",
    "    Creates label maps for encoding and decoding labels.\n",
    "\n",
    "    Args:\n",
    "        labels (numpy array): Array of label names.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (label_map, inverse_label_map)\n",
    "            label_map (dict): Mapping from label name to index.\n",
    "            inverse_label_map (dict): Mapping from index to label name.\n",
    "    \"\"\"\n",
    "    unique_labels = sorted(set(labels))\n",
    "    label_map = {name: idx for idx, name in enumerate(unique_labels)}\n",
    "    inverse_label_map = {idx: name for name, idx in label_map.items()}\n",
    "    return label_map, inverse_label_map\n",
    "\n",
    "\n",
    "def normalize_data(data, max_val):\n",
    "    \"\"\"\n",
    "    Normalizes the data by dividing by the maximum value.\n",
    "\n",
    "    Args:\n",
    "        data (numpy array): Data to normalize.\n",
    "        max_val (float): Maximum value for normalization.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Normalized data.\n",
    "    \"\"\"\n",
    "    return data / max_val if max_val != 0 else data"
   ],
   "id": "8b64c3f165fe955d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:28:19.087998Z",
     "start_time": "2024-12-03T14:28:19.086267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to your copick configuration\n",
    "config_path = '../../copick_config.json'"
   ],
   "id": "b5e99052b5a75998",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:28:19.415482Z",
     "start_time": "2024-12-03T14:28:19.098387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =======================\n",
    "# Training Phase\n",
    "# =======================\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "train_patches, train_labels = process_dataset(config_path, dataset_type='train')\n",
    "\n",
    "if train_patches.size == 0:\n",
    "    raise ValueError(\"No training patches were extracted. Check your Picks folder and data paths.\")\n",
    "\n",
    "print(f\"Extracted {train_patches.shape[0]} training patches.\")\n",
    "\n",
    "# Normalize patches based on global max from training data\n",
    "global_max = train_patches.max()\n",
    "train_patches = normalize_data(train_patches, global_max)\n",
    "\n",
    "# Encode labels\n",
    "label_map, inverse_label_map = prepare_label_maps(train_labels)\n",
    "encoded_train_labels = np.array([label_map[label] for label in train_labels])\n",
    "\n",
    "print(f\"Label mapping: {label_map}\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_patches, encoded_train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Add a channel dimension for CNN\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")"
   ],
   "id": "615692b1888ebc19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Picks folder /Users/jake.brannigan/Documents/Kaggle/CryoET/Data/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/0 does not exist. Skipping tomogram 0.\n",
      "Picks folder /Users/jake.brannigan/Documents/Kaggle/CryoET/Data/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/1 does not exist. Skipping tomogram 1.\n",
      "Picks folder /Users/jake.brannigan/Documents/Kaggle/CryoET/Data/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/2 does not exist. Skipping tomogram 2.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No training patches were extracted. Check your Picks folder and data paths.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m train_patches, train_labels \u001B[38;5;241m=\u001B[39m process_dataset(config_path, dataset_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m train_patches\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo training patches were extracted. Check your Picks folder and data paths.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExtracted \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_patches\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m training patches.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Normalize patches based on global max from training data\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: No training patches were extracted. Check your Picks folder and data paths."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =======================\n",
    "# Model Definition and Training\n",
    "# =======================\n",
    "\n",
    "# Define the 3D CNN model\n",
    "model = Sequential([\n",
    "    Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(16, 16, 16, 1)),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_map), activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'particle_detection_model.h5'\n",
    "# model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ],
   "id": "fc788e4e61b7e17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =======================\n",
    "# Prediction Phase\n",
    "# =======================\n",
    "\n",
    "def sliding_window_predict(model, tomogram, patch_size=16, step=8, threshold=0.5, label_map=None, global_max=1.0):\n",
    "    \"\"\"\n",
    "    Performs sliding window predictions on the test tomogram.\n",
    "\n",
    "    Args:\n",
    "        model (Keras model): Trained CNN model.\n",
    "        tomogram (numpy array): 3D tomogram volume.\n",
    "        patch_size (int): Size of cubic patches.\n",
    "        step (int): Step size for sliding window.\n",
    "        threshold (float): Confidence threshold for predictions.\n",
    "        label_map (dict): Map of label indices to particle names.\n",
    "        global_max (float): Global maximum value for normalization.\n",
    "\n",
    "    Returns:\n",
    "        list: Predicted particles with their positions and labels.\n",
    "    \"\"\"\n",
    "    half_size = patch_size // 2\n",
    "    z_max, y_max, x_max = tomogram.shape\n",
    "\n",
    "    predictions = []\n",
    "    for z in tqdm(range(half_size, z_max - half_size, step), desc=\"Z-axis\"):\n",
    "        for y in range(half_size, y_max - half_size, step):\n",
    "            for x in range(half_size, x_max - half_size, step):\n",
    "                patch = tomogram[z - half_size:z + half_size,\n",
    "                        y - half_size:y + half_size,\n",
    "                        x - half_size:x + half_size]\n",
    "                patch = patch[np.newaxis, ..., np.newaxis]  # Add batch and channel dimensions\n",
    "                patch = normalize_data(patch, global_max)  # Normalize using training global max\n",
    "\n",
    "                pred = model.predict(patch, verbose=0)\n",
    "                max_prob = np.max(pred)\n",
    "                label_idx = np.argmax(pred)\n",
    "\n",
    "                if max_prob > threshold:\n",
    "                    predictions.append({\n",
    "                        \"position\": (int(x), int(y), int(z)),\n",
    "                        \"label\": label_map[label_idx],\n",
    "                        \"confidence\": float(max_prob)\n",
    "                    })\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def prepare_test_data_multiple(config_path, dataset_type='test', num_tomograms=5):\n",
    "    \"\"\"\n",
    "    Prepares multiple test tomograms for predictions.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the copick configuration file.\n",
    "        dataset_type (str): Dataset type ('test').\n",
    "        num_tomograms (int): Number of tomograms to process.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tomogram data arrays.\n",
    "        list: List of tomogram keys.\n",
    "    \"\"\"\n",
    "    copick_root = copick.from_file(config_path)\n",
    "    runs = copick_root.runs\n",
    "    run = copick_root.get_run(runs[0].name)\n",
    "\n",
    "    voxel_spacing = run.get_voxel_spacing(10.000)\n",
    "\n",
    "    # Access the specific tomogram\n",
    "    tomogram = voxel_spacing.get_tomogram(\"denoised\")\n",
    "\n",
    "    # Access the Zarr data\n",
    "    zarr_store = tomogram.zarr()\n",
    "    zarr_group = zarr.open(zarr_store)\n",
    "\n",
    "    # Get tomogram keys and limit to first `num_tomograms`\n",
    "    all_keys = list(zarr_group.keys())\n",
    "    selected_keys = all_keys[:num_tomograms]\n",
    "\n",
    "    tomogram_data = []\n",
    "    for key in selected_keys:\n",
    "        tomogram_vals = zarr_group[key][:]\n",
    "        tomogram_data.append(tomogram_vals)\n",
    "\n",
    "    return tomogram_data, selected_keys\n",
    "\n",
    "\n",
    "def save_predictions(predictions, output_path):\n",
    "    \"\"\"\n",
    "    Saves predictions in overlay format.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): List of predicted particles with positions and labels.\n",
    "        output_path (str): Path to save the JSON file.\n",
    "    \"\"\"\n",
    "    overlay_data = {\n",
    "        \"points\": [\n",
    "            {\n",
    "                \"location\": {\"x\": pos[0], \"y\": pos[1], \"z\": pos[2]},\n",
    "                \"label\": label,\n",
    "                \"confidence\": confidence\n",
    "            }\n",
    "            for pred in predictions\n",
    "            for pos, label, confidence in [(pred[\"position\"], pred[\"label\"], pred[\"confidence\"])]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(overlay_data, json_file, indent=4)\n",
    "\n",
    "\n",
    "# Function to predict on multiple tomograms\n",
    "def predict_on_multiple_tomograms(model, config_path, num_tomograms=5, patch_size=16, step=8, threshold=0.5,\n",
    "                                  output_dir='predictions'):\n",
    "    \"\"\"\n",
    "    Predicts particle locations on multiple tomograms and saves the results.\n",
    "\n",
    "    Args:\n",
    "        model (Keras model): Trained CNN model.\n",
    "        config_path (str): Path to the copick configuration file.\n",
    "        num_tomograms (int): Number of tomograms to process.\n",
    "        patch_size (int): Size of cubic patches.\n",
    "        step (int): Step size for sliding window.\n",
    "        threshold (float): Confidence threshold for predictions.\n",
    "        output_dir (str): Directory to save prediction JSON files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare test data\n",
    "    tomograms, tomogram_keys = prepare_test_data_multiple(config_path, dataset_type='test', num_tomograms=num_tomograms)\n",
    "\n",
    "    if not tomograms:\n",
    "        print(\"No tomograms found for prediction.\")\n",
    "        return\n",
    "\n",
    "    for idx, (tomogram_vals, tomogram_key) in enumerate(zip(tomograms, tomogram_keys)):\n",
    "        print(f\"\\nPredicting on tomogram {idx + 1}/{num_tomograms}: {tomogram_key}\")\n",
    "\n",
    "        # Perform prediction\n",
    "        predictions = sliding_window_predict(\n",
    "            model=model,\n",
    "            tomogram=tomogram_vals,\n",
    "            patch_size=patch_size,\n",
    "            step=step,\n",
    "            threshold=threshold,\n",
    "            label_map=inverse_label_map,\n",
    "            global_max=global_max\n",
    "        )\n",
    "\n",
    "        # Save predictions\n",
    "        output_path = os.path.join(output_dir, f'predictions_{tomogram_key}.json')\n",
    "        save_predictions(predictions, output_path)\n",
    "        print(f\"Saved predictions to {output_path}. Detected {len(predictions)} particles.\")\n"
   ],
   "id": "d0fc4e58fb1399ee"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =======================\n",
    "# Execute Prediction on First 5 Tomograms\n",
    "# =======================\n",
    "\n",
    "# Define parameters\n",
    "NUM_TOMOGRAMS = 5  # Number of tomograms to predict on\n",
    "PATCH_SIZE = 16\n",
    "STEP = 8\n",
    "THRESHOLD = 0.5\n",
    "OUTPUT_DIR = 'predictions'\n",
    "\n",
    "# Load the trained model (optional, if not already in memory)\n",
    "# model = load_model('particle_detection_model.h5')\n",
    "\n",
    "# Perform predictions\n",
    "print(\"Starting predictions on test tomograms...\")\n",
    "predict_on_multiple_tomograms(\n",
    "    model=model,\n",
    "    config_path=config_path,\n",
    "    num_tomograms=NUM_TOMOGRAMS,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    step=STEP,\n",
    "    threshold=THRESHOLD,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "print(\"Prediction completed.\")\n"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
